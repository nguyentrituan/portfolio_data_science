# WELCOME üëãüèº

I‚Äôm a Data Analyst (Ex-Business Intelligence Analyst) based in Ho Chi Minh City with a passion for discovering **customer/product insights** through data analytics. Leveraging my background in International Business Economics, I help data-driven companies tell actionable stories.

# PORTFOLIO DATA SCIENCE 
## A/B Testing

In my experience working at leading tech companies, I've noticed that A/B testing is widely used, especially in companies and industries with large user bases such as e-commerce and gaming.

Why use A/B testing? With a large user base, companies cannot simply apply any feature they want, as doing so may carry the risk of hurting user experience and subsequently affecting the lifetime value of users. Therefore, A/B testing is necessary to control risks. Instead of applying changes to the entire user base, companies can apply them to a small group, for example, about 10% of users, and then use statistical methods to determine if there is a statistically significant difference between the control group and the variants.

In terms of methodology, I have documented it in this [Gitbook Link](https://app.gitbook.com/o/VfRPaxLWrwVO1zxPDj2s/s/iggGa4mab2uKFxO1Zj5M/). On GitHub, I provide code for both frequentist and Bayesian approaches in A/B testing.

Files consist of the following parts:
1. Frequentist Code
2. Bayesian Code

![image](https://github.com/nguyentrituan/portfolio_data_science/assets/121095339/59bb93a6-e043-41b7-ba45-e98128be6b99)


## Kaggle Competition - Prediction Interval for Birth Weight

I have had the opportunity to participate in a Kaggle competition, and we ranked in the top 10 on the leaderboard.

Kaggle Competition Info https://www.kaggle.com/competitions/prediction-interval-competition-i-birth-weight/overview

Problem: In many ML contests involving regression it is not unusual to focus on what are called point predictions (expectation values) ùëåÃÇ  However, in this competition we are uninterested in the expectation value, but rather in producing a prediction interval, ùê∂ùõº^ . Indeed prediction intervals are often of greater practical interest than point predictions, which only form part of the story. In this competition we shall be predicting the intervals associated with birth weights.
Goal: To obtain the minimum average prediction interval.

In the competition, I experimented with various techniques ranging from simple to complex to tune models, including KNN, Linear Regression, XGBoost Regressor, Decision Tree, and Random Forest. The results showed that the XGBoost model performed the best.

Note:
XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks. However, when it comes to small-to-medium structured/tabular data, decision tree based algorithms are considered best-in-class right now. XGBoost and Gradient Boosting Machines (GBMs) are both ensemble tree methods that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture. However, XGBoost improves upon the base GBM framework through systems optimization and algorithmic enhancements.

![image](https://github.com/nguyentrituan/portfolio_data_science/assets/121095339/e17796bd-3fc9-48fc-bd90-fbfedf4a7793)


# Machine Learning for Marketing

ƒê√¢y l√† kho√° h·ªçc m√† t√¥i ƒë∆∞·ª£c c·∫•p ch·ª©ng ch·ªâ t·∫°i Trung T√¢m tin h·ªçc ƒê·∫°i h·ªçc Khoa h·ªçc t·ª± nhi√™n

N√≥ b·∫£o g·ªìm r·∫•t nhi·ªÅu Model v·ªÅ Machine Learning, ƒë·∫∑c bi·ªát c√≥ ch·ª©a ph·∫ßn thi t·ªët nghi·ªáp c·ªßa t√¥i trong kho√° h·ªçc

# Segmentation

Bao g·ªìm 3 c√°ch segmentation g·ªìm K-means, Cohort view v√† RFM

